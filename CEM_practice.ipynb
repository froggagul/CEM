{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-gcGPBoy4K"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D679oMEoYsG",
        "outputId": "eaeaa7d0-7cfc-4706-ab6c-9aa1378ff7cc"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\" pygame matplotlib ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16k8nmQuod_P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from IPython.display import HTML, Javascript, display\n",
        "\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7peCCmyno2Ys"
      },
      "source": [
        "# CEM algorithm implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1aG6oDEogUo"
      },
      "outputs": [],
      "source": [
        "class CEM:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        bounds: np.ndarray,\n",
        "        population_size: int = 500,\n",
        "        elite_frac: float = 0.1,\n",
        "        max_iters: int = 50,\n",
        "        smoothing: float = 0.7,\n",
        "        seed: int = None,\n",
        "    ):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.pop_size = population_size\n",
        "        self.n_elite = max(1, int(np.ceil(elite_frac * population_size)))\n",
        "        self.max_iters = max_iters\n",
        "        self.alpha = smoothing\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.best_obj_history = []\n",
        "\n",
        "    def optimize(self, x_init: np.ndarray, objective):\n",
        "        mu = x_init.copy()\n",
        "        sigma = (self.bounds[1] - self.bounds[0]) / 2.0\n",
        "\n",
        "        self.best_obj_history = []\n",
        "\n",
        "        for i in range(self.max_iters):\n",
        "            pass\n",
        "\n",
        "            # 1. random sample x and clip with bounds\n",
        "\n",
        "            # 2. query y values from sampled x\n",
        "\n",
        "            # 3. get top k minimum (x,y)\n",
        "\n",
        "            # 3. calculate mean, std from top k samples\n",
        "\n",
        "            # 3. update mean and sigma\n",
        "\n",
        "            # [optional] For debug, log best y values and mean y values, etc ..\n",
        "\n",
        "        # return final mean\n",
        "        return mu\n",
        "\n",
        "    def visualize(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(1, len(self.best_obj_history) + 1), self.best_obj_history, marker='o')\n",
        "        plt.title(\"Best Value per Iteration\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Objective (Lower is Better)\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objective function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs9SiMezp_8d"
      },
      "outputs": [],
      "source": [
        "def objective_vec(train_env, batch: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generic vectorized discrete-action objective for CEM (to be MINIMIZED).\n",
        "    Works for CartPole (action_dim=2) or LunarLander (action_dim=4) or any other discrete gym env.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_env : VectorEnv\n",
        "        A vectorized gymnasium environment.\n",
        "    batch : np.ndarray, shape (N, D)\n",
        "        Population of parameter vectors. Each θ has length\n",
        "            obs_dim * action_dim  +  action_dim\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, shape (N,)\n",
        "        Negative average reward per candidate (CEM will minimize).\n",
        "    \"\"\"\n",
        "    N, D = batch.shape\n",
        "    obs_dim = train_env.single_observation_space.shape[0]\n",
        "    action_dim = train_env.single_action_space.n\n",
        "\n",
        "    # sanity check\n",
        "    assert D == obs_dim * action_dim + action_dim, (\n",
        "        f\"Expected θ to be length {obs_dim*action_dim + action_dim}, \"\n",
        "        f\"got {D}\"\n",
        "    )\n",
        "\n",
        "    results = np.zeros(N, dtype=float)\n",
        "\n",
        "    for i in range(N):\n",
        "        theta = batch[i]\n",
        "        # 1. unpack θ → W, b\n",
        "        W = None\n",
        "        b = None\n",
        "\n",
        "        # 2. get sum of rewards with policy and env\n",
        "        # Note: sum of rewards should be maximized, but CEM works to minimize objective. So we should change the sign.\n",
        "        # gym env usage:\n",
        "        # single step:\n",
        "        #    obs, rewards, terminated, truncated, _ = train_env.step(actions)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function - evaluation\n",
        "\n",
        "You don't need to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nwwEosfvgCv"
      },
      "outputs": [],
      "source": [
        "def animate_policy(\n",
        "    theta: np.ndarray,\n",
        "    env_name: str = \"CartPole-v1\",\n",
        "    max_steps: int = 500,\n",
        "    interval: int = 100,\n",
        "    goal_reward: float = None\n",
        ") -> HTML:\n",
        "    \"\"\"\n",
        "    Render a discrete-action policy for any Gymnasium env and display a reward curve\n",
        "    with an optional goal_reward marker.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    theta : np.ndarray, shape=(obs_dim*action_dim + action_dim,)\n",
        "        Flattened policy parameters.\n",
        "    env_name : str\n",
        "        Gym environment ID.\n",
        "    max_steps : int\n",
        "        Max timesteps to run.\n",
        "    interval : int\n",
        "        Milliseconds between animation frames.\n",
        "    goal_reward : float or None\n",
        "        If provided, draw a horizontal line at this reward and a vertical line\n",
        "        at the first step where cumulative reward ≥ goal_reward.\n",
        "    \"\"\"\n",
        "    # Setup env\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert theta.size == obs_dim*action_dim + action_dim, \\\n",
        "        f\"θ length should be {obs_dim*action_dim + action_dim}, got {theta.size}\"\n",
        "    W = theta[:obs_dim*action_dim].reshape(obs_dim, action_dim)\n",
        "    b = theta[obs_dim*action_dim:]\n",
        "\n",
        "    # Rollout and collect frames & rewards\n",
        "    obs, _ = env.reset()\n",
        "    frames, rewards = [], []\n",
        "    total = 0.0\n",
        "    goal_step = None\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        logits = obs.dot(W) + b\n",
        "        action = int(np.argmax(logits))\n",
        "        obs, r, terminated, truncated, _ = env.step(action)\n",
        "        total += r\n",
        "        rewards.append(total)\n",
        "        if goal_reward is not None and goal_step is None and total >= goal_reward:\n",
        "            goal_step = t+1\n",
        "        if terminated or truncated:\n",
        "            frames.append(env.render())\n",
        "            rewards.append(total)\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    # Encode frames\n",
        "    uris = []\n",
        "    for f in frames:\n",
        "        img = Image.fromarray(f)\n",
        "        buf = io.BytesIO()\n",
        "        img.save(buf, format=\"PNG\")\n",
        "        uris.append(\"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode())\n",
        "\n",
        "    # Plot reward curve with goal markers\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(len(rewards)), rewards, marker='o')\n",
        "    if goal_reward is not None:\n",
        "        ax.axhline(goal_reward, color='red', linestyle='--', label=f\"Goal = {goal_reward}\")\n",
        "        if goal_step is not None:\n",
        "            ax.axvline(goal_step, color='green', linestyle='--',\n",
        "                       label=f\"Reached at t={goal_step}\")\n",
        "        ax.legend()\n",
        "    ax.set_title(\"Cumulative Reward\")\n",
        "    ax.set_xlabel(\"Step\")\n",
        "    ax.set_ylabel(\"Total Reward\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"PNG\")\n",
        "    plt.close(fig)\n",
        "    graph_uri = \"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode()\n",
        "\n",
        "    # Build HTML\n",
        "    total_frames = len(frames)\n",
        "    w = frames[0].shape[1]\n",
        "    h = frames[0].shape[0]\n",
        "\n",
        "    charset = string.ascii_letters + string.digits\n",
        "    chars = np.array(list(charset))\n",
        "    picks = np.random.choice(chars, size=6)\n",
        "    # Join them into one string\n",
        "    frame_id = ''.join(picks)\n",
        "\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <div style=\"text-align:center;\">\n",
        "      <img id=\"cem_frame_{frame_id}\" width=\"{w}\" height=\"{h}\" style=\"border:1px solid #333\"/>\n",
        "      <p id=\"frame_counter_{frame_id}\">Frame: 1 / {total_frames}</p>\n",
        "      <div>\n",
        "        <img src=\"{graph_uri}\" width=\"400\"/>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "    js = f\"\"\"\n",
        "    const frames = [{','.join(f\"'{u}'\" for u in uris)}];\n",
        "    let idx = 0, total = frames.length;\n",
        "    const img = document.getElementById('cem_frame_{frame_id}'),\n",
        "          counter = document.getElementById('frame_counter_{frame_id}');\n",
        "    img.src = frames[0];\n",
        "    setInterval(() => {{\n",
        "      idx = (idx + 1) % total;\n",
        "      img.src = frames[idx];\n",
        "      counter.innerText = 'Frame: ' + (idx+1) + ' / ' + total;\n",
        "    }}, {interval});\n",
        "    \"\"\"\n",
        "    display(Javascript(js))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AfNR4_0qglU"
      },
      "source": [
        "# Optimization example : Cartpole\n",
        "- [Gymnasium docs](https://gymnasium.farama.org/v0.29.0/environments/classic_control/cart_pole/)\n",
        "- Takes about 30 sec ~ 1 min to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1DPRX8JogPO",
        "outputId": "e9b06c0d-c6b2-409e-ee4f-38a8c7404c08"
      },
      "outputs": [],
      "source": [
        "train_env = gym.make_vec(\n",
        "    \"CartPole-v1\",\n",
        "    num_envs = 3,\n",
        "    vectorization_mode = \"vector_entry_point\"\n",
        ")\n",
        "obs_dim = train_env.single_observation_space.shape[0]\n",
        "action_dim = train_env.single_action_space.n\n",
        "objective = lambda batch: objective_vec(train_env, batch)\n",
        "\n",
        "# You need to define\n",
        "# 1. dimension of linear policy weight\n",
        "# 2. bound where we should search policy\n",
        "# 3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "dim = None\n",
        "bounds = None\n",
        "population_size = None\n",
        "elite_frac = None\n",
        "max_iters = None\n",
        "smoothing = None\n",
        "x0 = None\n",
        "\n",
        "cem = CEM(dim=dim, bounds=bounds, population_size=population_size, elite_frac=elite_frac, max_iters=max_iters, smoothing=smoothing)\n",
        "\n",
        "best_theta = cem.optimize(\n",
        "    x0,\n",
        "    objective,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test if your policy is working well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "uqUubxdqvkTg",
        "outputId": "7cad1ff2-325c-4e6a-887a-2c397d12d185"
      },
      "outputs": [],
      "source": [
        "animate_policy(best_theta, max_steps=500, interval=100, env_name='CartPole-v1', goal_reward=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Om53i3q9Rw"
      },
      "source": [
        "# Optimization example : Lunar Lander\n",
        "- [Gymnasium docs](https://gymnasium.farama.org/v0.29.0/environments/box2d/lunar_lander/)\n",
        "- Takes about 10 min to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btNYtIWVqPeI",
        "outputId": "ec15fe90-203c-4a13-abf7-1f7c3730e947"
      },
      "outputs": [],
      "source": [
        "train_env = gym.make_vec(\n",
        "    \"LunarLander-v3\",\n",
        "    num_envs = 3,\n",
        ")\n",
        "\n",
        "# You need to define\n",
        "# 1. dimension of linear policy weight\n",
        "# 2. bound where we should search policy\n",
        "# 3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "obs_dim = train_env.single_observation_space.shape[0]\n",
        "action_dim = train_env.single_action_space.n\n",
        "objective = lambda batch: objective_vec(train_env, batch)\n",
        "\n",
        "# You need to define\n",
        "# 1. dimension of linear policy weight\n",
        "# 2. bound where we should search policy\n",
        "# 3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "dim = None\n",
        "bounds = None\n",
        "population_size = None\n",
        "elite_frac = None\n",
        "max_iters = None\n",
        "smoothing = None\n",
        "x0 = None\n",
        "\n",
        "cem = CEM(dim=dim, bounds=bounds, population_size=population_size, elite_frac=elite_frac, max_iters=max_iters, smoothing=smoothing)\n",
        "\n",
        "best_theta = cem.optimize(\n",
        "    x0,\n",
        "    objective,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test if your policy is working well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "qqOuTL3MtnkB",
        "outputId": "4c045714-0057-41af-bd8c-ffce40d33061"
      },
      "outputs": [],
      "source": [
        "animate_policy(best_theta, max_steps=500, interval=100, env_name='LunarLander-v3', goal_reward=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpr08q6RvSmg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
